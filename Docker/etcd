ectd: A distributed, reliable key-value store for the most critical data of a distributed system.

More information:
https://coreos.com/etcd/
https://www.infoq.cn/article/etcd-interpretation-application-scenario-implement-principle           # in Chinese
https://coreos.com/etcd/docs/latest/v2/clustering.html          # Clustering Guide



---------------------------------------  Introduction ------------------------------------------------------------------
etcd is a distributed key value store that provides a reliable way to store data across a cluster of machines. 
It’s open-source and available on GitHub. etcd gracefully handles leader elections during network partitions and 
will tolerate machine failure, including the leader.

Your applications can read and write data into etcd. A simple use-case is to store database connection details or 
feature flags in etcd as key value pairs. These values can be watched, allowing your app to reconfigure itself when they change.
Advanced uses take advantage of the consistency guarantees to implement database leader elections or do distributed locking 
across a cluster of workers.

etcd 作为一个受到 ZooKeeper 与 doozer 启发而催生的项目，除了拥有与之类似的功能外，更专注于以下四点。
简单：基于 HTTP+JSON 的 API 让你用 curl 就可以轻松使用。
安全：可选 SSL 客户认证机制。
快速：每个实例每秒支持一千次写操作。
可信：使用 Raft 算法充分实现了分布式。

值得注意的是，分布式系统中的数据分为控制数据和应用数据。使用 etcd 的场景默认处理的数据都是控制数据，
对于应用数据，只推荐数据量很小，但是更新访问频繁的情况。


---------------------------------------  Install and Examples for V2 -----------------------------------------------------------
More information: https://coreos.com/etcd/docs/latest/getting-started-with-etcd.html

$ apt-get install etcd
$ apt-get install jq                         # a command line tool for json file processor.
$ etcd -version                              # v2 or v3
$ etcd                                       # the following examples is for v2

$ etcdctl set /message Hello
Hello
$ etcdctl get /message
Hello
$ etcdctl rm /message
$ etcdctl mkdir /foo-service
Cannot print key [/foo-service: Is a directory]
$ etcdctl set /foo-service/container1 localhost:1111
localhost:1111
$ etcdctl ls /foo-service
/foo-service/container1
$ etcdctl watch --recursive /foo-service

$ curl -s http://127.0.0.1:2379/v2/keys/message -X PUT -d value="Hello world" | jq .            # set value          
$ curl -s http://127.0.0.1:2379/v2/keys/message | jq .                                          # get value          
$ curl -s http://127.0.0.1:2379/v2/keys/message -X PUT -d value="Hello etcd" | jq .             # change value
$ curl -s http://127.0.0.1:2379/v2/keys/message -X DELETE | jq .
$ curl -s http://127.0.0.1:2379/v2/keys/foo -X PUT -d value=bar -d ttl=5 |jq .
$ curl -s http://127.0.0.1:2379/v2/keys/foo?wait=true
$ curl -s http://127.0.0.1:2379/v2/keys/dir -XPUT -d dir=true | jq .
$ curl -s http://127.0.0.1:2379/v2/keys/dir
$ curl -s http://127.0.0.1:2379/v2/keys/dir?recursive=true
$ curl -s http://127.0.0.1:2379/v2/keys/dir?recursive=true&wait=true
$ curl -s http://127.0.0.1:2379/v2/keys/dir?dir=true -XDELETE


---------------------------------------  Install and Examples for V3 ---------------------------------------------------------
$ curl -L https://github.com/etcd-io/etcd/releases/download/v3.3.11/etcd-v3.3.11-linux-amd64.tar.gz -o etcd-v3.3.11.tar.gz
$ tar xzvf etcd-v3.3.11.tar.gz && cd etcd-v3.3.11
$ cp etcd /usr/bin
$ cp etcdctl /usr/bin
$ etcd --version
$ etcdctl --version
$ etcd
$ export ETCDCTL_API=3
$ etcdctl put foo "hello world"
$ etcdctl get foo


--------------------------------------- Version Differences ------------------------------------------------------------------
获得了 IANA 认证的端口，2379 用于客户端通信，2380 用于节点通信，与原先的（4001 peers / 7001 clients）共用。
每个节点可监听多个广播地址。监听的地址由原来的一个扩展到多个，用户可以根据需求实现更加复杂的集群环境，如一个是公网 IP，
一个是虚拟机（容器）之类的私有 IP。
etcd 集群和集群中的节点都有了自己独特的 ID。这样就防止出现配置混淆，不是本集群的其他 etcd 节点发来的请求将被屏蔽。
etcd 集群启动时的配置信息目前变为完全固定，这样有助于用户正确配置和启动。

新版区别于旧版的特性: 
-- 获得了 IANA 认证的端口，2379 用于客户端通信，2380 用于节点通信，与原先的（4001 peers / 7001 clients）共用。
-- 它摒弃了使用配置文件进行参数配置的做法，转而使用命令行参数或者环境变量的做法来配置参数。
-- 每个节点可监听多个广播地址。监听的地址由原来的一个扩展到多个，用户可以根据需求实现更加复杂的集群环境，如一个是公网 IP，
   一个是虚拟机（容器）之类的私有 IP。
-- etcd可以代理访问leader节点的请求，如果你可以访问任何一个etcd节点，那么你就可以无视网络的拓扑结构对整个集群进行读写操作。
-- etcd 集群和集群中的节点都有了自己独特的 ID。这样就防止出现配置混淆，不是本集群的其他 etcd 节点发来的请求将被屏蔽。
-- 废弃集群自动调整功能的 standby 模式，这个功能使得用户维护集群更困难。
-- 新增 Proxy 模式，不加入到 etcd 一致性集群中，纯粹进行代理转发。
-- ETCD_NAME（-name）参数目前是可选的，不再用于唯一标识一个节点。
-- 过自发现方式启动集群必须要提供集群大小，这样有助于用户确定集群实际启动的节点数量。

etcd概念词汇：
-- Raft：etcd 所采用的保证分布式系统强一致性的算法。
-- Node：一个 Raft 状态机实例。
-- Member： 一个 etcd 实例。它管理着一个 Node，并且可以为客户端请求提供服务。
-- Cluster：由多个 Member 构成可以协同工作的 etcd 集群。
-- Peer：对同一个 etcd 集群中另外一个 Member 的称呼。
-- Client： 向 etcd 集群发送 HTTP 请求的客户端。
-- WAL：预写式日志，etcd 用于持久化存储的日志格式。
-- snapshot：etcd 防止 WAL 文件过多而设置的快照，存储 etcd 数据状态。
-- Proxy：etcd 的一种模式，为 etcd 集群提供反向代理服务。
-- Leader：Raft 算法中通过竞选而产生的处理所有数据提交的节点。
-- Follower：竞选失败的节点作为 Raft 中的从属节点，为算法提供强一致性保证。
-- Candidate：当 Follower 超过一定时间接收不到 Leader 的心跳时转变为 Candidate 开始竞选。
-- Term：某个节点成为 Leader 到下一次竞选时间，称为一个 Term。
-- Index：数据项编号。Raft 中通过 Term 和 Index 来定位数据。



---------------------------------------  Cluster Configuration ------------------------------------------------------------------
192.168.10.147 # master节点(etcd,kubernetes-master)
192.168.10.148 # node节点(etcd,kubernetes-node,docker,flannel)
192.168.10.149 # node节点(etcd,kubernetes-node,docker,flannel)

systemd file: /usr/bin/systemd/system/etcd.service

for master:
$ vi /etc/etcd/etcd.conf
# [member]
ETCD_NAME=etcd1
ETCD_DATA_DIR="/var/lib/etcd/etcd1.etcd"
ETCD_LISTEN_PEER_URLS="http://192.168.10.147:2380"
ETCD_LISTEN_CLIENT_URLS="http://192.168.10.147:2379,http://127.0.0.1:2379"
CD_MAX_SNAPSHOTS="5"
#
#[cluster]
ETCD_INITIAL_ADVERTISE_PEER_URLS="http://192.168.10.147:2380"
# if you use different ETCD_NAME (e.g. test), set ETCD_INITIAL_CLUSTER value for this name, i.e. "test=http://..."
ETCD_INITIAL_CLUSTER="etcd1=http://192.168.10.147:2380,etcd2=http://192.168.10.148:2380,etcd3=http://192.168.10.149:2380"
ETCD_ADVERTISE_CLIENT_URLS="http://192.168.10.147:2379"

for node1
vi /etc/etcd/etcd.conf
# [member]
ETCD_NAME=etcd2
ETCD_DATA_DIR="/var/lib/etcd/etcd2"
ETCD_LISTEN_PEER_URLS="http://192.168.10.148:2380"
ETCD_LISTEN_CLIENT_URLS="http://192.168.10.148:2379,http://127.0.0.1:2379"
#
#[cluster]
ETCD_INITIAL_ADVERTISE_PEER_URLS="http://192.168.10.148:2380"
# if you use different ETCD_NAME (e.g. test), set ETCD_INITIAL_CLUSTER value for this name, i.e. "test=http://..."
ETCD_INITIAL_CLUSTER="etcd1=http://192.168.10.147:2380,etcd2=http://192.168.10.148:2380,etcd3=http://192.168.10.149:2380"
ETCD_ADVERTISE_CLIENT_URLS="http://192.168.10.148:2379"

for node2
...

至此etcd集群就部署完了，然后每个节点上启动
$ systemctl start kube-apiserver
验证：
[root@k8s1 ~]# etcdctl cluster-health
member 35300bfb5308e02c is healthy: got healthy result from http://192.168.10.147:2379
member 776c306b60e6f972 is healthy: got healthy result from http://192.168.10.149:2379
member a40f86f061be3fbe is healthy: got healthy result from http://192.168.10.148:2379


针对几个URLS做下简单的解释：
[member]
ETCD_NAME ：ETCD的节点名
ETCD_DATA_DIR：ETCD的数据存储目录
ETCD_SNAPSHOT_COUNTER：多少次的事务提交将触发一次快照
ETCD_HEARTBEAT_INTERVAL：ETCD节点之间心跳传输的间隔，单位毫秒
ETCD_ELECTION_TIMEOUT：该节点参与选举的最大超时时间，单位毫秒
ETCD_LISTEN_PEER_URLS：该节点与其他节点通信时所监听的地址列表，多个地址使用逗号隔开，其格式可以划分为scheme://IP:PORT，
这里的scheme可以是http、https
ETCD_LISTEN_CLIENT_URLS：该节点与客户端通信时监听的地址列表
[cluster]
ETCD_INITIAL_ADVERTISE_PEER_URLS：该成员节点在整个集群中的通信地址列表，这个地址用来传输集群数据的地址。
因此这个地址必须是可以连接集群中所有的成员的。
ETCD_INITIAL_CLUSTER：配置集群内部所有成员地址，其格式为：ETCD_NAME=ETCD_INITIAL_ADVERTISE_PEER_URLS，如果有多个使用逗号隔开
ETCD_ADVERTISE_CLIENT_URLS：广播给集群中其他成员自己的客户端地址列表

------------------------------  Configuration by Environment Variables and Startup arguments ---------------------------------------

在每个 etcd 机器启动时，配置环境变量或者添加启动参数的方式如下。
ETCD_INITIAL_CLUSTER="infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380"
ETCD_INITIAL_CLUSTER_STATE=new

参数方法：
-initial-cluster 
infra0=http://10.0.1.10:2380,http://10.0.1.11:2380,infra2=http://10.0.1.12:2380 \
 -initial-cluster-state new
值得注意的是，-initial-cluster参数中配置的 url 地址必须与各个节点启动时设置的initial-advertise-peer-urls参数相同。
（initial-advertise-peer-urls参数表示节点监听其他节点同步信号的地址）
如果你所在的网络环境配置了多个 etcd 集群，为了避免意外发生，最好使用-initial-cluster-token参数为每个集群单独配置一个 token 认证。
这样就可以确保每个集群和集群的成员都拥有独特的 ID。

综上所述，如果你要配置包含 3 个 etcd 节点的集群，那么你在三个机器上的启动命令分别如下所示。
$ etcd -name infra0 -initial-advertise-peer-urls http://10.0.1.10:2380 \
  -listen-peer-urls http://10.0.1.10:2380 \
  -initial-cluster-token etcd-cluster-1 \
  -initial-cluster infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380 \
  -initial-cluster-state new

$ etcd -name infra1 -initial-advertise-peer-urls http://10.0.1.11:2380 \
  -listen-peer-urls http://10.0.1.11:2380 \
  -initial-cluster-token etcd-cluster-1 \
  -initial-cluster infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380 \
  -initial-cluster-state new

$ etcd -name infra2 -initial-advertise-peer-urls http://10.0.1.12:2380 \
  -listen-peer-urls http://10.0.1.12:2380 \
  -initial-cluster-token etcd-cluster-1 \
  -initial-cluster infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380 \
  -initial-cluster-state new


---------------------------------------  Raft consensus algorithm ------------------------------------------------------------------
More information:
https://www.brianstorti.com/raft/
https://www.geeksforgeeks.org/raft-consensus-algorithm/
https://raft.github.io/
https://www.cnblogs.com/softidea/p/6517959.html     #中文介绍


Introduction:
Raft protocol was developed by Diego Ongaro and John Ousterhout (Stanford University) which won Diego his Ph.D in 2014. 
Raft was designed for better understandability of how Consensus can be achieved considering that its predecessor, the Paxos Algorithm, 
developed by Lesli Lamport is very difficult to understand and implement. Before Raft, Paxos was considered the holy grail 
in achieving Consensus.

Consensus:
Consensus means multiple servers agreeing on same information, something imperative to design fault-tolerant distributed systems.

A consensus protocol tolerating failures must have the following features :
Validity : If a process decides(read/write) a value, then it must have been proposed by some other correct process
Agreement : Every correct process must agree on the same value
Termination : Every correct process must terminate after a finite number of steps.
Integrity : If all correct processes decide on the same value, then any process has the said value.

We shall now define some terms used to refer individual servers in a distributed system.
Leader – Only the server elected as leader can interact with the client. All other servers sync up themselves with the leader. 
         At any point of time, there can be at most one leader(possibly 0, which we shall explain later)
Follower – Follower servers sync up their copy of data with that of the leader’s after every regular time intervals. 
           When the leader server goes down(due to any reason), one of the followers can contest an election and become the leader.
Candidate – At the time of contesting an election to choose the leader server, the servers can ask other servers for votes. 
            Hence, they are called candidates when they have requested votes. Initially, all servers are in the Candidate state.
        
        
Raft协议主要分为三个部分：选主，日志复制，安全性。
5.1 选主
Raft协议是用于维护一组服务节点数据一致性的协议。这一组服务节点构成一个集群，并且有一个主节点来对外提供服务。当集群初始化，或者主节点挂掉后，
面临一个选主问题。集群中每个节点，任意时刻处于Leader, Follower, Candidate这三个角色之一。选举特点如下：

当集群初始化时候，每个节点都是Follower角色；
集群中存在至多1个有效的主节点，通过心跳与其他节点同步数据；
当Follower在一定时间内没有收到来自主节点的心跳，会将自己角色改变为Candidate，并发起一次选主投票；当收到包括自己在内超过半数节点赞成后，
选举成功；当收到票数不足半数选举失败，或者选举超时。若本轮未选出主节点，将进行下一轮选举（出现这种情况，是由于多个节点同时选举，所有节点均为
获得过半选票）。
Candidate节点收到来自主节点的信息后，会立即终止选举过程，进入Follower角色。

为了避免陷入选主失败循环，每个节点未收到心跳发起选举的时间是一定范围内的随机值，这样能够避免2个节点同时发起选主。

5.2 日志复制
所谓日志复制，是指主节点将每次操作形成日志条目，并持久化到本地磁盘，然后通过网络IO发送给其他节点。其他节点根据日志的逻辑时钟(TERM)和
日志编号(INDEX)来判断是否将该日志记录持久化到本地。当主节点收到包括自己在内超过半数节点成功返回，那么认为该日志是可提交的(committed），
并将日志输入到状态机，将结果返回给客户端。

这里需要注意的是，每次选主都会形成一个唯一的TERM编号，相当于逻辑时钟。每一条日志都有全局唯一的编号。

主节点通过网络IO向其他节点追加日志。若某节点收到日志追加的消息，首先判断该日志的TERM是否过期，以及该日志条目的INDEX是否比当前以及提交的
日志的INDEX跟早。若已过期，或者比提交的日志更早，那么就拒绝追加，并返回该节点当前的已提交的日志的编号。否则，将日志追加，并返回成功。

当主节点收到其他节点关于日志追加的回复后，若发现有拒绝，则根据该节点返回的已提交日志编号，发生其编号下一条日志。

主节点像其他节点同步日志，还作了拥塞控制。具体地说，主节点发现日志复制的目标节点拒绝了某次日志追加消息，将进入日志探测阶段，一条一条发送日志，
直到目标节点接受日志，然后进入快速复制阶段，可进行批量日志追加。

按照日志复制的逻辑，我们可以看到，集群中慢节点不影响整个集群的性能。另外一个特点是，数据只从主节点复制到Follower节点，这样大大简化了逻辑流程。

5.3 安全性
截止此刻，选主以及日志复制并不能保证节点间数据一致。试想，当一个某个节点挂掉了，一段时间后再次重启，并当选为主节点。而在其挂掉这段时间内，
集群若有超过半数节点存活，集群会正常工作，那么会有日志提交。这些提交的日志无法传递给挂掉的节点。当挂掉的节点再次当选主节点，它将缺失部分
已提交的日志。在这样场景下，按Raft协议，它将自己日志复制给其他节点，会将集群已经提交的日志给覆盖掉。

这显然是不可接受的。

其他协议解决这个问题的办法是，新当选的主节点会询问其他节点，和自己数据对比，确定出集群已提交数据，然后将缺失的数据同步过来。这个方案有明显缺陷，
增加了集群恢复服务的时间（集群在选举阶段不可服务），并且增加了协议的复杂度。

Raft解决的办法是，在选主逻辑中，对能够成为主的节点加以限制，确保选出的节点已定包含了集群已经提交的所有日志。如果新选出的主节点已经包含了集群
所有提交的日志，那就不需要从和其他节点比对数据了。简化了流程，缩短了集群恢复服务的时间。

这里存在一个问题，加以这样限制之后，还能否选出主呢？答案是：只要仍然有超过半数节点存活，这样的主一定能够选出。因为已经提交的日志必然被集群中
超过半数节点持久化，显然前一个主节点提交的最后一条日志也被集群中大部分节点持久化。当主节点挂掉后，集群中仍有大部分节点存活，那这存活的节点中
一定存在一个节点包含了已经提交的日志了。

From: https://blog.csdn.net/zl1zl2zl3/article/details/79627412
Etcd，Zookeeper，Consul 比较
这三个产品是经常被人拿来做选型比较的。 
Etcd 和 Zookeeper 提供的能力非常相似，都是通用的一致性元信息存储，都提供watch机制用于变更通知和分发，也都被分布式系统用来作为共享信息存储，
在软件生态中所处的位置也几乎是一样的，可以互相替代的。二者除了实现细节，语言，一致性协议上的区别，最大的区别在周边生态圈。
Zookeeper 是apache下的，用java写的，提供rpc接口，最早从hadoop项目中孵化出来，在分布式系统中得到广泛使用（hadoop, solr, kafka, mesos 等）。
Etcd 是coreos公司旗下的开源产品，比较新，以其简单好用的rest接口以及活跃的社区俘获了一批用户，在新的一些集群中得到使用（比如kubernetes）。
虽然v3为了性能也改成二进制rpc接口了，但其易用性上比 Zookeeper 还是好一些。 而 Consul 的目标则更为具体一些，Etcd 和 Zookeeper 提供的是
分布式一致性存储能力，具体的业务场景需要用户自己实现，比如服务发现，比如配置变更。而Consul 则以服务发现和配置变更为主要目标，同时附带了kv存储。
在软件生态中，越抽象的组件适用范围越广，但同时对具体业务场景需求的满足上肯定有不足之处。           


